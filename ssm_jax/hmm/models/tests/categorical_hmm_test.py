"""
https://github.com/hmmlearn/hmmlearn/blob/main/lib/hmmlearn/tests/test_categorical_hmm.py
"""
import jax.numpy as jnp
import jax.random as jr
import pytest
from jax import vmap
from ssm_jax.hmm.learning import hmm_fit_em
from ssm_jax.hmm.models.base import BaseHMM
from ssm_jax.hmm.models.categorical_hmm import CategoricalHMM


def normalize(a, axis=None):
    """
    Normalize the input array so that it sums to 1.
    Parameters
    ----------
    a : array
        Non-normalized input data.
    axis : int
        Dimension along which normalization is performed.
    Notes
    -----
    Modifies the input **inplace**.
    """
    a_sum = a.sum(axis)
    if axis and a.ndim > 1:
        # Make sure we don't divide by zero.
        a_sum[a_sum == 0] = 1
        shape = list(a.shape)
        shape[axis] = 1
        a_sum.shape = shape

    return a / a_sum


def normalized(X, axis=None):
    return normalize(X, axis=axis)


def new_hmm():
    '''
    States : rainy, sunny
    Emissions : walk, shop, clean
    '''
    initial_probabilities = jnp.array([0.6, 0.4])
    transition_matrix = jnp.array([[0.7, 0.3], [0.4, 0.6]])
    emission_probs = jnp.array([[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])
    hmm = CategoricalHMM(initial_probabilities, transition_matrix, emission_probs)
    return hmm


class TestCategoricalAgainstWikipedia:
    """
    Examples from Wikipedia:
    - http://en.wikipedia.org/wiki/Hidden_Markov_model
    - http://en.wikipedia.org/wiki/Viterbi_algorithm
    """

    def test_decode_viterbi(self):
        # From http://en.wikipedia.org/wiki/Viterbi_algorithm:
        # "This reveals that the observations ['walk', 'shop', 'clean']
        #  were most likely generated by states ['Sunny', 'Rainy', 'Rainy'],
        #  with probability 0.01344."
        hmm = new_hmm()
        X = jnp.array([[0], [1], [2]])
        state_sequence = vmap(hmm.most_likely_states)(X)
        # assert round(jnp.exp(log_prob), 5) == 0.01344
        assert jnp.allclose(jnp.squeeze(state_sequence), jnp.array([1, 0, 0]))

    def test_predict(self):
        X = jnp.array([[0], [1], [2]])
        hmm = new_hmm()
        posteriors = vmap(hmm.filter)(X)
        filtered_probs = jnp.squeeze(posteriors.filtered_probs, axis=1)
        print(filtered_probs)
        assert jnp.allclose(filtered_probs,
                            jnp.array([
                                [0.23170303, 0.76829697],
                                [0.62406281, 0.37593719],
                                [0.86397706, 0.13602294],
                            ]),
                            atol=1e-1)


class TestCategoricalHMM:

    def setup(self):
        self.num_components = 2
        self.num_features = 3

    def test_score_samples(self, key=jr.PRNGKey(0)):
        idx = jnp.repeat(jnp.arange(self.num_components), 10)
        num_samples = len(idx)
        emissions = jr.randint(key, shape=(num_samples, 1), minval=0, maxval=self.num_features - 1)
        hmm = new_hmm()

        posterior = vmap(hmm.filter)(emissions)
        filtered_probs = posterior.filtered_probs
        predicted_probs = posterior.predicted_probs

        assert filtered_probs.shape == (num_samples, 1, self.num_components)
        assert predicted_probs.shape == (num_samples, 1, self.num_components)

        assert jnp.allclose(filtered_probs.sum(axis=-1), jnp.ones((num_samples, 1)), atol=1e-6, rtol=1e-6)
        assert jnp.allclose(predicted_probs.sum(axis=-1), jnp.ones((num_samples, 1)), atol=1e-6, rtol=1e-6)

    def test_sample(self, key=jr.PRNGKey(0), num_samples=1000):
        hmm = new_hmm()
        state_sequence, emissions = hmm.sample(key, num_samples)
        assert len(emissions) == len(state_sequence) == num_samples
        assert len(jnp.unique(emissions)) == self.num_features

    def test_em(self, key=jr.PRNGKey(0), num_states=4, num_samples=1000):
        sample_key, init_key = jr.split(key)

        true_initial_probs = jnp.ones((num_states,)) / (num_states * 1.)
        true_transition_matrix = 0.90 * jnp.eye(num_states) + 0.10 * jnp.ones((num_states, num_states)) / num_states
        true_emission_probs = 0.90 * jnp.eye(num_states) + 0.10 * jnp.ones((num_states, num_states)) / num_states
        true_hmm = CategoricalHMM(true_initial_probs, true_transition_matrix, true_emission_probs)
        state_sequence, emissions = true_hmm.sample(sample_key, num_samples)

        hmm = CategoricalHMM.random_initialization(init_key, num_states, num_states)
        res1, lps1 = hmm_fit_em(hmm, emissions[None, ...])
        hmm.m_step = BaseHMM.m_step
        res2, lps2 = hmm_fit_em(hmm, emissions[None, ...])

        assert jnp.allclose(res1.emission_probs, res2.emission_probs)
        assert jnp.allclose(res1.initial_probabilities, res2.initial_probabilities)
        assert jnp.allclose(jnp.array(lps1), jnp.array(lps2))
